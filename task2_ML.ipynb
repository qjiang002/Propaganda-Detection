{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "['765982381', '727736557', '741655444', '762546428', '696735702']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "file_number = []\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.endswith(\".txt\") :\n",
    "        file_number.append(re.findall(r'\\d+', filename)[0])\n",
    "        \n",
    "file_number = list(set(file_number))\n",
    "print(len(file_number))\n",
    "print(file_number[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 293 articles in total.\n",
      "There are 15170 sentences in total.\n",
      "There are 3938 propaganda sentences in total.\n",
      "There are 10325 non-propaganda sentences in total.\n"
     ]
    }
   ],
   "source": [
    "article_sentence,sentence_number,gold_labels = ([],[],[])\n",
    "article_dic = {}\n",
    "sent_cnt = 0\n",
    "propaganda_cnt = 0\n",
    "\n",
    "propaganda_list = []\n",
    "non_propaganda_list = []\n",
    "\n",
    "for file_num in file_number:\n",
    "    article_dic[file_num] = {}\n",
    "    with open(file_path+\"/\"+\"article\"+file_num+\".txt\", \"r\", encoding='utf-8') as f:\n",
    "        count=1\n",
    "        for line in f.readlines():\n",
    "            #line = line.strip()\n",
    "            #if len(line)>0:\n",
    "            article_dic[file_num][str(count)] = {'text':line.strip()}\n",
    "            count+=1\n",
    "    #print(\"length of article %s is %d\" % (file_num, len(article_dic[file_num])))\n",
    "    sent_cnt += len(article_dic[file_num])\n",
    "    with open(file_path+\"/\"+\"article\"+file_num+\".task2.labels\", \"r\", encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            article_id, sentence_id, gold_label = line.rstrip().split(\"\\t\")\n",
    "            if gold_label=='propaganda':\n",
    "                propaganda_cnt+=1\n",
    "            if article_dic[str(article_id)][str(sentence_id)]['text']=='':\n",
    "                continue\n",
    "            if str(sentence_id) not in article_dic[str(article_id)]:\n",
    "                print(\"not found: %s\" % (str(sentence_id)))\n",
    "            else:\n",
    "                article_dic[str(article_id)][str(sentence_id)]['label'] = gold_label\n",
    "                if gold_label=='propaganda':\n",
    "                    propaganda_list.append((article_dic[str(article_id)][str(sentence_id)]['text'],'propaganda'))\n",
    "                else:\n",
    "                    non_propaganda_list.append((article_dic[str(article_id)][str(sentence_id)]['text'],'non_propaganda'))\n",
    "            \n",
    "print(\"There are %d articles in total.\" % len(article_dic))\n",
    "print(\"There are %d sentences in total.\" % sent_cnt)\n",
    "print(\"There are %d propaganda sentences in total.\" % len(propaganda_list))\n",
    "print(\"There are %d non-propaganda sentences in total.\" % len(non_propaganda_list))\n",
    "#print(\"Among them, there are %d sentences labeled as propaganda.\\n\" % gold_labels.count('propaganda'))\n",
    "#print(\"Example sentence:\\n\", sent_dic[2])\n",
    "#print(\"Label:\", gold_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set: 11410\n",
      "length of dev set: 942\n",
      "length of test set: 1911\n"
     ]
    }
   ],
   "source": [
    "sentences = propaganda_list+non_propaganda_list\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "test_sentences, dev_sentences = train_test_split(test_sentences, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"length of train set: %d\"%(len(train_sentences)))\n",
    "print(\"length of dev set: %d\"%(len(dev_sentences)))\n",
    "print(\"length of test set: %d\"%(len(test_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set propaganda label: 3143\n",
      "dev set propaganda label: 263\n",
      "test set propaganda label: 532\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for item in train_sentences:\n",
    "    if item[1]=='propaganda':\n",
    "        cnt+=1\n",
    "print(\"train set propaganda label: %d\"%(cnt))\n",
    "cnt=0\n",
    "for item in dev_sentences:\n",
    "    if item[1]=='propaganda':\n",
    "        cnt+=1\n",
    "print(\"dev set propaganda label: %d\"%(cnt))\n",
    "cnt=0\n",
    "for item in test_sentences:\n",
    "    if item[1]=='propaganda':\n",
    "        cnt+=1\n",
    "print(\"test set propaganda label: %d\"%(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11410\n",
      "1911\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "for item in train_sentences:\n",
    "    x_train.append(item[0])\n",
    "    y_train.append(item[1])\n",
    "        \n",
    "for item in test_sentences:\n",
    "    x_test.append(item[0])\n",
    "    y_test.append(item[1])\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 11410, n_features: 28496\n",
      "Checking that the number of features in train and test correspond: 28496 - 28496\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.5, max_features=None, \n",
    "                             strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1,3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "train = vectorizer.fit_transform(x_train)\n",
    "print(\"n_samples: %d, n_features: %d\" % train.shape)\n",
    "test = vectorizer.transform(x_test)\n",
    "#print(test)\n",
    "#feature_name = vectorizer.get_feature_names()\n",
    "print(\"Checking that the number of features in train and test correspond: %s - %s\" % (train.shape[1],test.shape[1]))\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.81909161 0.48529412]\n",
      "recall: [0.7715736  0.55827068]\n",
      "fscore: [0.79462285 0.51923077]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7121925693354265\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "model = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=40)\n",
    "model.fit(train, y_train)\n",
    "predictions = model.predict(test)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.75988539 0.68072289]\n",
      "recall: [0.96156635 0.21240602]\n",
      "fscore: [0.84891165 0.32378223]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7530088958660387\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    " \n",
    "classifier = LogisticRegression() \n",
    "classifier.fit(train, y_train)  \n",
    "\n",
    "predictions = classifier.predict(test) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74577227 0.59124088]\n",
      "recall: [0.95939086 0.15225564]\n",
      "fscore: [0.83920076 0.24215247]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7346938775510204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, n_estimators = 8)\n",
    "rf.fit(train, y_train)\n",
    "predictions = rf.predict(test) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74431503 0.65740741]\n",
      "recall: [0.97316896 0.13345865]\n",
      "fscore: [0.84349466 0.221875  ]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7394034536891679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "rf_20 = RandomForestClassifier(oob_score=True, n_estimators = 20)\n",
    "rf_20.fit(train, y_train)\n",
    "predictions = rf_20.predict(test) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74055829 0.69047619]\n",
      "recall: [0.98114576 0.10902256]\n",
      "fscore: [0.84404242 0.18831169]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7383568812140241\n"
     ]
    }
   ],
   "source": [
    "rf_100 = RandomForestClassifier(oob_score=True, n_estimators = 100)\n",
    "rf_100.fit(train, y_train)\n",
    "predictions = rf_100.predict(test) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k:4, best score:0.7310, best method: distance\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "best_k=-1\n",
    "best_score=0\n",
    "best_method=''\n",
    "for method in ['uniform','distance']:\n",
    "    for i in range(1,11):\n",
    "        knn_clf=KNeighborsClassifier(n_neighbors=i,weights=method)\n",
    "        knn_clf.fit(train,y_train)\n",
    "        scores=knn_clf.score(test,y_test)\n",
    "        if scores>best_score:\n",
    "            best_score=scores\n",
    "            best_k=i\n",
    "            best_method=method\n",
    "\n",
    "print('best k:%d, best score:%.4f, best method: %s'%(best_k,best_score,best_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.73265196 0.67307692]\n",
      "recall: [0.98767223 0.06578947]\n",
      "fscore: [0.84126004 0.11986301]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7310308738880168\n"
     ]
    }
   ],
   "source": [
    "knn_clf=KNeighborsClassifier(n_neighbors=4,weights='distance')\n",
    "knn_clf.fit(train,y_train)\n",
    "predictions = knn_clf.predict(test) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i]=='propaganda':\n",
    "        cnt+=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11410, 28370)\n",
      "(11410, 28382)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.5, max_features=None, \n",
    "                             strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1,3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "x_train_vec = vectorizer.fit_transform(x_train).toarray()\n",
    "print(x_train_vec.shape)\n",
    "\n",
    "ner_list = ['PERSON','LOC','ORG','DATE','TIME','NORP','PRODUCT','EVENT','PERCENT','QUANTITY','ORDINAL','CARDINAL']\n",
    "features = []\n",
    "for i, item in enumerate(x_train):\n",
    "    feature = []\n",
    "    doc = nlp(item)\n",
    "    doc = [X.label_ for X in doc.ents]\n",
    "    for ner in ner_list:\n",
    "        if ner in doc:\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    features.append(feature)\n",
    "features = np.array(features)\n",
    "x_train_vec = np.concatenate((x_train_vec, features), axis=1)\n",
    "print(x_train_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1911, 28370)\n",
      "(1911, 28382)\n"
     ]
    }
   ],
   "source": [
    "x_test_vec = vectorizer.transform(x_test).toarray()\n",
    "print(x_test_vec.shape)\n",
    "\n",
    "ner_list = ['PERSON','LOC','ORG','DATE','TIME','NORP','PRODUCT','EVENT','PERCENT','QUANTITY','ORDINAL','CARDINAL']\n",
    "features = []\n",
    "for i, item in enumerate(x_test):\n",
    "    feature = []\n",
    "    doc = nlp(item)\n",
    "    doc = [X.label_ for X in doc.ents]\n",
    "    for ner in ner_list:\n",
    "        if ner in doc:\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    features.append(feature)\n",
    "features = np.array(features)\n",
    "x_test_vec = np.concatenate((x_test_vec, features), axis=1)\n",
    "print(x_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.84034268 0.5215311 ]\n",
      "recall: [0.78245105 0.61466165]\n",
      "fscore: [0.81036425 0.56427955]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7357404500261643\n"
     ]
    }
   ],
   "source": [
    "#SVM using NER feature\n",
    "model = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=40)\n",
    "model.fit(x_train_vec, y_train)\n",
    "predictions = model.predict(x_test_vec)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.77381648 0.725     ]\n",
      "recall: [0.96011603 0.27255639]\n",
      "fscore: [0.85695793 0.39617486]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7687074829931972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression using NER feature\n",
    "from sklearn.linear_model import LogisticRegression \n",
    " \n",
    "classifier = LogisticRegression() \n",
    "classifier.fit(x_train_vec, y_train)  \n",
    "\n",
    "predictions = classifier.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74890351 0.85057471]\n",
      "recall: [0.99057288 0.13909774]\n",
      "fscore: [0.85295036 0.23909532]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7535321821036107\n"
     ]
    }
   ],
   "source": [
    "#RandomForest_100 using NER feature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf_100 = RandomForestClassifier(oob_score=True, n_estimators = 100)\n",
    "rf_100.fit(x_train_vec, y_train)\n",
    "predictions = rf_100.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.75653207 0.53744493]\n",
      "recall: [0.92385787 0.22932331]\n",
      "fscore: [0.83186419 0.32147563]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7305075876504448\n"
     ]
    }
   ],
   "source": [
    "#KNN using NER feature\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier(n_neighbors=4,weights='distance')\n",
    "knn_clf.fit(x_train_vec,y_train)\n",
    "predictions = knn_clf.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11410, 28370)\n",
      "(1911, 28370)\n",
      "(11410, 28374)\n",
      "(1911, 28374)\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(x_train).toarray()\n",
    "print(x_train_vec.shape)\n",
    "x_test_vec = vectorizer.transform(x_test).toarray()\n",
    "print(x_test_vec.shape)\n",
    "\n",
    "features = []\n",
    "for i, item in enumerate(x_train):\n",
    "    polarity_score = vader_analyzer.polarity_scores(item)\n",
    "    feature = [score for (name, score) in polarity_score.items()]\n",
    "    features.append(feature)\n",
    "    \n",
    "features = np.array(features)\n",
    "x_train_vec = np.concatenate((x_train_vec, features), axis=1)\n",
    "print(x_train_vec.shape)\n",
    "\n",
    "features = []\n",
    "for i, item in enumerate(x_test):\n",
    "    polarity_score = vader_analyzer.polarity_scores(item)\n",
    "    feature = [score for (name, score) in polarity_score.items()]\n",
    "    features.append(feature)\n",
    "    \n",
    "features = np.array(features)\n",
    "x_test_vec = np.concatenate((x_test_vec, features), axis=1)\n",
    "print(x_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.84073507 0.53553719]\n",
      "recall: [0.79622915 0.60902256]\n",
      "fscore: [0.81787709 0.56992084]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7441130298273155\n"
     ]
    }
   ],
   "source": [
    "#SVM using polarity_score\n",
    "model = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=40)\n",
    "model.fit(x_train_vec, y_train)\n",
    "predictions = model.predict(x_test_vec)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.77189888 0.68571429]\n",
      "recall: [0.95213923 0.27067669]\n",
      "fscore: [0.8525974  0.38814016]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7624280481423339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression using polarity_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    " \n",
    "classifier = LogisticRegression() \n",
    "classifier.fit(x_train_vec, y_train)  \n",
    "\n",
    "predictions = classifier.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74536532 0.84415584]\n",
      "recall: [0.99129804 0.12218045]\n",
      "fscore: [0.85091815 0.2134647 ]\n",
      "support: [1379  532]\n",
      "accuracy: 0.749345892203035\n"
     ]
    }
   ],
   "source": [
    "#RandomForest_100 using polarity_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf_100 = RandomForestClassifier(oob_score=True, n_estimators = 100)\n",
    "rf_100.fit(x_train_vec, y_train)\n",
    "predictions = rf_100.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74694105 0.68141593]\n",
      "recall: [0.97389413 0.14473684]\n",
      "fscore: [0.84545168 0.23875969]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7430664573521716\n"
     ]
    }
   ],
   "source": [
    "#KNN using polarity_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier(n_neighbors=4,weights='distance')\n",
    "knn_clf.fit(x_train_vec,y_train)\n",
    "predictions = knn_clf.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11410, 28370)\n",
      "(1911, 28370)\n",
      "(11410, 28386)\n",
      "(1911, 28386)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.5, max_features=None, \n",
    "                             strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1,3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "x_train_vec = vectorizer.fit_transform(x_train).toarray()\n",
    "print(x_train_vec.shape)\n",
    "x_test_vec = vectorizer.transform(x_test).toarray()\n",
    "print(x_test_vec.shape)\n",
    "\n",
    "ner_list = ['PERSON','LOC','ORG','DATE','TIME','NORP','PRODUCT','EVENT','PERCENT','QUANTITY','ORDINAL','CARDINAL']\n",
    "features = []\n",
    "for i, item in enumerate(x_train):\n",
    "    feature = []\n",
    "    doc = nlp(item)\n",
    "    doc = [X.label_ for X in doc.ents]\n",
    "    for ner in ner_list:\n",
    "        if ner in doc:\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    features.append(feature)\n",
    "features = np.array(features)\n",
    "x_train_vec = np.concatenate((x_train_vec, features), axis=1)\n",
    "\n",
    "\n",
    "features = []\n",
    "for i, item in enumerate(x_test):\n",
    "    feature = []\n",
    "    doc = nlp(item)\n",
    "    doc = [X.label_ for X in doc.ents]\n",
    "    for ner in ner_list:\n",
    "        if ner in doc:\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    features.append(feature)\n",
    "features = np.array(features)\n",
    "x_test_vec = np.concatenate((x_test_vec, features), axis=1)\n",
    "\n",
    "features = []\n",
    "for i, item in enumerate(x_train):\n",
    "    polarity_score = vader_analyzer.polarity_scores(item)\n",
    "    feature = [score for (name, score) in polarity_score.items()]\n",
    "    features.append(feature)\n",
    "    \n",
    "features = np.array(features)\n",
    "x_train_vec = np.concatenate((x_train_vec, features), axis=1)\n",
    "print(x_train_vec.shape)\n",
    "\n",
    "features = []\n",
    "for i, item in enumerate(x_test):\n",
    "    polarity_score = vader_analyzer.polarity_scores(item)\n",
    "    feature = [score for (name, score) in polarity_score.items()]\n",
    "    features.append(feature)\n",
    "    \n",
    "features = np.array(features)\n",
    "x_test_vec = np.concatenate((x_test_vec, features), axis=1)\n",
    "print(x_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.8404908  0.53377265]\n",
      "recall: [0.79477883 0.60902256]\n",
      "fscore: [0.8169959  0.56892011]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7430664573521716\n"
     ]
    }
   ],
   "source": [
    "#SVM using NER & polarity_score\n",
    "model = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=40)\n",
    "model.fit(x_train_vec, y_train)\n",
    "predictions = model.predict(x_test_vec)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.7742704  0.65948276]\n",
      "recall: [0.94271211 0.28759398]\n",
      "fscore: [0.85022891 0.40052356]\n",
      "support: [1379  532]\n",
      "accuracy: 0.760334903192046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangqi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression using NER & polarity_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    " \n",
    "classifier = LogisticRegression() \n",
    "classifier.fit(x_train_vec, y_train)  \n",
    "\n",
    "predictions = classifier.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.74726477 0.84337349]\n",
      "recall: [0.99057288 0.13157895]\n",
      "fscore: [0.8518865  0.22764228]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7514390371533228\n"
     ]
    }
   ],
   "source": [
    "#RandomForest_100 using NER & polarity_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf_100 = RandomForestClassifier(oob_score=True, n_estimators = 100)\n",
    "rf_100.fit(x_train_vec, y_train)\n",
    "predictions = rf_100.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.76181818 0.53256705]\n",
      "recall: [0.91153009 0.2612782 ]\n",
      "fscore: [0.82997689 0.35056747]\n",
      "support: [1379  532]\n",
      "accuracy: 0.7305075876504448\n"
     ]
    }
   ],
   "source": [
    "#KNN using NER & polarity_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier(n_neighbors=4,weights='distance')\n",
    "knn_clf.fit(x_train_vec,y_train)\n",
    "predictions = knn_clf.predict(x_test_vec) \n",
    "precision,recall,fscore,support=precision_recall_fscore_support(y_test, predictions)\n",
    "print('precision: {0}'.format(precision))\n",
    "print('recall: {0}'.format(recall))\n",
    "print('fscore: {0}'.format(fscore))\n",
    "print('support: {0}'.format(support))\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
